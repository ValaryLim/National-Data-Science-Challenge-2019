{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classifier </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1: Setting up the Notebook </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import machine learing algorithms\n",
    "from sklearn import model_selection, naive_bayes, svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# Import accuracy checker\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import pickle to save models\n",
    "import pickle\n",
    "\n",
    "# Import matplotlib.pyplot for graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2: Data Exploration </h3>\n",
    "<p> We first performed some descriptive analytics to better understand the train and test data that we were working with. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>title</th>\n",
       "      <th>Category</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307504</td>\n",
       "      <td>nyx sex bomb pallete natural palette</td>\n",
       "      <td>0</td>\n",
       "      <td>beauty_image/6b2e9cbb279ac95703348368aa65da09.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461203</td>\n",
       "      <td>etude house precious mineral any cushion pearl...</td>\n",
       "      <td>1</td>\n",
       "      <td>beauty_image/20450222d857c9571ba8fa23bdedc8c9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3592295</td>\n",
       "      <td>milani rose powder blush</td>\n",
       "      <td>2</td>\n",
       "      <td>beauty_image/6a5962bed605a3dd6604ca3a4278a4f9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4460167</td>\n",
       "      <td>etude house baby sweet sugar powder</td>\n",
       "      <td>3</td>\n",
       "      <td>beauty_image/56987ae186e8a8e71fcc5a261ca485da.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5853995</td>\n",
       "      <td>bedak revlon color stay aqua mineral make up</td>\n",
       "      <td>3</td>\n",
       "      <td>beauty_image/9c6968066ebab57588c2f757a240d8b9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    itemid                                              title  Category  \\\n",
       "0   307504               nyx sex bomb pallete natural palette         0   \n",
       "1   461203  etude house precious mineral any cushion pearl...         1   \n",
       "2  3592295                           milani rose powder blush         2   \n",
       "3  4460167                etude house baby sweet sugar powder         3   \n",
       "4  5853995       bedak revlon color stay aqua mineral make up         3   \n",
       "\n",
       "                                          image_path  \n",
       "0  beauty_image/6b2e9cbb279ac95703348368aa65da09.jpg  \n",
       "1  beauty_image/20450222d857c9571ba8fa23bdedc8c9.jpg  \n",
       "2  beauty_image/6a5962bed605a3dd6604ca3a4278a4f9.jpg  \n",
       "3  beauty_image/56987ae186e8a8e71fcc5a261ca485da.jpg  \n",
       "4  beauty_image/9c6968066ebab57588c2f757a240d8b9.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>title</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370855998</td>\n",
       "      <td>flormar 7 white cream bb spf 30 40ml</td>\n",
       "      <td>beauty_image/1588591395c5a254bab84042005f2a9f.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>637234604</td>\n",
       "      <td>maybelline clear smooth all in one bb cream sp...</td>\n",
       "      <td>beauty_image/920985ed9587ea20f58686ea74e20f93.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>690282890</td>\n",
       "      <td>murah innisfree eco natural green tea bb cream...</td>\n",
       "      <td>beauty_image/90b40e5710f54352b243fcfb0f5d1d7f.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>930913462</td>\n",
       "      <td>loreal white perfect day cream spf 17 pa white...</td>\n",
       "      <td>beauty_image/289c668ef3d70e1d929d602d52d5d78a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1039280071</td>\n",
       "      <td>hada labo cc cream ultimate anti aging spf 35 ...</td>\n",
       "      <td>beauty_image/d5b3e652c5822d2306f4560488ec30c6.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       itemid                                              title  \\\n",
       "0   370855998               flormar 7 white cream bb spf 30 40ml   \n",
       "1   637234604  maybelline clear smooth all in one bb cream sp...   \n",
       "2   690282890  murah innisfree eco natural green tea bb cream...   \n",
       "3   930913462  loreal white perfect day cream spf 17 pa white...   \n",
       "4  1039280071  hada labo cc cream ultimate anti aging spf 35 ...   \n",
       "\n",
       "                                          image_path  \n",
       "0  beauty_image/1588591395c5a254bab84042005f2a9f.jpg  \n",
       "1  beauty_image/920985ed9587ea20f58686ea74e20f93.jpg  \n",
       "2  beauty_image/90b40e5710f54352b243fcfb0f5d1d7f.jpg  \n",
       "3  beauty_image/289c668ef3d70e1d929d602d52d5d78a.jpg  \n",
       "4  beauty_image/d5b3e652c5822d2306f4560488ec30c6.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We observe that the test data has the same columns as the train data except that it is missing the \"Category\" column that we are meant to create with our predictive model. </p>\n",
    "\n",
    "<p> Moreover, the title data has some stopwords, e.g. \"in\" and \"all\" that are unlikely to have much predictive value for our model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.666150e+05</td>\n",
       "      <td>666615.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.155562e+09</td>\n",
       "      <td>18.071577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.226888e+08</td>\n",
       "      <td>13.090931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.125740e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.120021e+08</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.252422e+09</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.612608e+09</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.868917e+09</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             itemid       Category\n",
       "count  6.666150e+05  666615.000000\n",
       "mean   1.155562e+09      18.071577\n",
       "std    5.226888e+08      13.090931\n",
       "min    1.125740e+05       0.000000\n",
       "25%    8.120021e+08       4.000000\n",
       "50%    1.252422e+09      18.000000\n",
       "75%    1.612608e+09      28.000000\n",
       "max    1.868917e+09      57.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.724020e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.152965e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.472060e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.126550e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.791964e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.282126e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.631265e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.868894e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             itemid\n",
       "count  1.724020e+05\n",
       "mean   1.152965e+09\n",
       "std    5.472060e+08\n",
       "min    1.126550e+05\n",
       "25%    7.791964e+08\n",
       "50%    1.282126e+09\n",
       "75%    1.631265e+09\n",
       "max    1.868894e+09"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The title in the first row of the train dataset says \"palette\". While this word was in singular form, we expect that there would be some titles with \"palettes\" instead. We thus slice the train dataset to confirm our suspicion. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['title'].str.contains('palette')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['title'].str.contains('palettes')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We expect that there would be other words, e.g. \"color\" or \"colors\" that will have both singular and plural form as well. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <strong> Conclusions </strong> </p>\n",
    "<p> After looking at the title data, we identified the following issues with the title data that would reduce the accuracy of our machine learning model:\n",
    "<ol>\n",
    "    <li> Some title data contained punctuation, special characters and numbers that had little significance to the title data.</li>\n",
    "    <li> Some titles reflected products in plural and others in singular form, e.g. \"palette\" and \"palettes\" are the same thing but would be registed in a model as different. </li>\n",
    "    <li> There were a lot of stopwords, e.g. \"the\", \"and\", \"in\", that we do not expect to have much predictive value. </li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3: Data Pre-Processing </h3>\n",
    "<p> Having identified the above problems, we decided to run the title data through a pre-processing pipeline (mainly using the nltk library) to make the names easier to deal with. The following are steps in our pipeline: </p>\n",
    "\n",
    "<ol>\n",
    "    <li> Removing any blank rows in the data. </li>\n",
    "    <li> Changing all letters to lowercase, since python interprets 'color' and 'COLOR' differently. </li>\n",
    "    <li> Removing punctuation, special characters (like *, | or .) and numbers in the title data. </li>\n",
    "    <li> Lemmatizing the words (≈finding word stems) to remove variance from word inflection (i.e. we want our model to know that \"palette\" and \"palettes\" are the same thing) </li>\n",
    "    <li> Removing stopwords (the, and, in, etc.) because we do not expect them to have much predictive value. </li> \n",
    "</ol>\n",
    "\n",
    "<p> We also split the words in the title string in each row into a list of words to make it easier to parse through all words in the string. </P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove blank rows\n",
    "train_df['title'].dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Change all the text to lower case\n",
    "train_df['title'] = [entry.lower() for entry in train_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenization : In this each entry in the train_df will be broken into set of words\n",
    "train_df['title']= [word_tokenize(entry) for entry in train_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Remove stopwords, punctuationon, special characters and numeric data, and lemmatize the words (find word stems)\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun ozRr verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index,entry in enumerate(train_df['title']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    # The final processed set of words for each iteration will be stored in 'title_final'\n",
    "    train_df.loc[index,'title_final'] = str(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clean data for future use\n",
    "train_df.to_pickle(\"train_df_cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test data\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(train_df['title_final'], train_df['Category'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Extracting Features from Title Data </h4>\n",
    "<p> In order to run machine learning algorithms, we need to convert the titles into numerical feature vectors. We chose to use the Term Frequency - Inverse Document Frequency (TF-IDF) method that reduces the weightage of more common words that occur in all documents, e.g. color </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train_df['title_final'])\n",
    "train_X_Tfidf = Tfidf_vect.transform(train_X)\n",
    "test_X_Tfidf = Tfidf_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"train_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4: Machine Learning Models </h3>\n",
    "<p> We then tested out a few different supervised machine learning models and chose the one with the highest accuracy. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Logistic Regression </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegression = LogisticRegression()\n",
    "LogRegression.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression = LogRegression.predict(test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression Accuracy Score -> \", accuracy_score(predictions_LogRegression, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_LogReg = 'finalized_model_LogReg.sav'\n",
    "pickle.dump(LogRegression, open(filename_LogReg, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Naive Bayes Classifier Algorithm </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(train_X_Tfidf,train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_Naive = 'finalized_model_NaiveBayes.sav'\n",
    "pickle.dump(Naive, open(filename_Naive, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Support Vector Machine </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_SVM = 'finalized_model_SVM.sav'\n",
    "pickle.dump(SVM, open(filename_SVM, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Decision Tree </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "DecisionTree = tree.DecisionTreeClassifier()\n",
    "DecisionTree.fit(train_X_Tfidf, trian_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = DecisionTree.predict(text_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get accuracy\n",
    "print(\"Decision Tree Accuracy Score -> \", accuracy_score(predictions_DecisionTree, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_DecisionTree = 'finalized_model_DecisionTree.sav'\n",
    "pickle.dump(DecisionTree, open(filename_DecisionTree, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> K-Nearest Neighbors </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "KNN = KNeighborsClassifier(n_neighbors = 7)\n",
    "KNN.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN = KNN.predict(text_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get accuracy\n",
    "print(\"KNN Accuracy Score -> \", accuracy_score(predictions_KNN, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_KNN = 'finalized_model_KNN.sav'\n",
    "pickle.dump(KNN, open(filename_KNN, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Stochastic Gradient Descent </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SGD = SGDClassifier(loss = \"hinge\", penalty = \"12\", max_iter = 5)\n",
    "SGD.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SGD = SGD.predict(text_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get accuracy\n",
    "print(\"SGD Accuracy Score -> \", accuracy_score(predictions_SGD, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_SGD = 'finalized_model_SGD.sav'\n",
    "pickle.dump(SGD, open(filename_SGD, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5: Improving the Model </h3>\n",
    "<h4> Logistic Regression </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the Logistic Regression Model\n",
    "\n",
    "# account for class imbalances (if any)\n",
    "LogRegression2 = LogisticRegression(class_weight = 'balanced')\n",
    "LogRegression2.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression2 = LogRegression2.predict(test_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression V2 Accuracy Score -> \", accuracy_score(y_true=test_Y, y_pred=predictions_LogRegression2)*100)\n",
    "\n",
    "# final result\n",
    "# Logistic Regression V2 Accuracy Score ->  68.47013526014452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the Logistic Regression Model\n",
    "\n",
    "# changing the solver from the default of 'liblinear' which does not handle multinomial loss\n",
    "# note that 'lbfgs' generates a convergence warning\n",
    "LogRegression3 = LogisticRegression(solver = 'newton-cg')\n",
    "LogRegression3.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression3 = LogRegression3.predict(test_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression V3 Accuracy Score -> \", accuracy_score(y_true=test_Y, y_pred=predictions_LogRegression3)*100)\n",
    "\n",
    "# final result\n",
    "# Logistic Regression V3 Accuracy Score ->  71.07633072480436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the Logistic Regression Model\n",
    "\n",
    "# adding multi_class = 'auto'\n",
    "LogRegression4 = LogisticRegression(solver = 'newton-cg', multi_class = 'auto')\n",
    "LogRegression4.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression4 = LogRegression4.predict(test_X_Tfidf)\n",
    "\n",
    "# use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression V4 Accuracy Score -> \", accuracy_score(y_true=test_Y, y_pred=predictions_LogRegression4)*100)\n",
    "\n",
    "# final result\n",
    "# Logistic Regression V4 Accuracy Score ->  71.17683826286971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the Logistic Regression Model\n",
    "\n",
    "# changing the solver from the default of 'liblinear' \n",
    "LogRegression5 = LogisticRegression(solver = 'saga', multi_class = 'auto')\n",
    "LogRegression5.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression5 = LogRegression5.predict(test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression V5 Accuracy Score -> \", accuracy_score(y_true=test_Y, y_pred=predictions_LogRegression5)*100)\n",
    "\n",
    "# final result\n",
    "# Logistic Regression V5 Accuracy Score ->  71.1803385253894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the Logistic Regression Model\n",
    "\n",
    "# changing the solver from the default of 'liblinear' \n",
    "LogRegression6 = LogisticRegression(solver = 'sag', multi_class = 'auto')\n",
    "LogRegression6.fit(train_X_Tfidf, train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_LogRegression6 = LogRegression6.predict(test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression V6 Accuracy Score -> \", accuracy_score(y_true=test_Y, y_pred=predictions_LogRegression6)*100)\n",
    "\n",
    "# final result\n",
    "# Logistic Regression V6 Accuracy Score -> 71.17733830037253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The Logistic Regression using the SAGA solver and auto multiclass had the highest accuracy. Thus, we saved it and used it to predict the categories of data in 'test.csv'. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_LogReg = 'LogReg_SagaSolver.sav'\n",
    "pickle.dump(LogRegression5, open(filename_LogReg, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Using Entire 'train.csv' </h4>\n",
    "<p> After finding the best model, we chose to use all the train data to train the model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_df['title_final']\n",
    "train_Y = train_df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train_df['title_final'])\n",
    "train_X_Tfidf = Tfidf_vect.transform(train_X)\n",
    "test_Tfidf = Tfidf_vect.transform(test_df['title_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Regression with all data using solver = 'saga', multi_class = 'auto'\n",
    "LogRegressionAll = LogisticRegression(solver = 'saga', multi_class = 'auto')\n",
    "LogRegressionAll.fit(train_X_Tfidf, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_LogReg = 'LogReg_SagaSolver_All.sav'\n",
    "pickle.dump(LogRegressionAll, open(filename_LogReg, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6: Predictions </h3>\n",
    "<p> In order to make use of our machine learning models, we first need to process the test data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove blank rows\n",
    "test_df['title'].dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Change all the text to lower case\n",
    "test_df['title'] = [entry.lower() for entry in test_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenization : In this each entry in the train_df will be broken into set of words\n",
    "test_df['title']= [word_tokenize(entry) for entry in test_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove stopwords, punctuationon, special characters and numeric data, and lemmatize the words (find word stems)\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun ozRr verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index,entry in enumerate(test_df['title']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    # The final processed set of words for each iteration will be stored in 'title_final'\n",
    "    test_df.loc[index,'title_final'] = str(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"test_df_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2: Loading and Using the Models </h3>\n",
    "<p> We opened the models previously saved and then used those models to predict our test data. We then added the predictions as a 'Category' column to the existing dataframe. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open cleaned test dataframe\n",
    "with open('test_df_cleaned.pkl', 'rb') as test:\n",
    "    test_df = pickle.load(test)\n",
    "\n",
    "# Open cleaned train dataframe\n",
    "with open('train_df_cleaned.pickle', 'rb') as train:\n",
    "    train_df = pickle.load(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open final model\n",
    "with open('LogReg_SagaSolver_All.sav', 'rb') as f:\n",
    "    LogReg_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train_df['title_final'])\n",
    "test_Tfidf = Tfidf_vect.transform(test_df['title_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories\n",
    "predictions_LogReg = LogReg_model.predict(test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted cateogries to test_df\n",
    "test_df['Category'] = predictions_LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the predictions\n",
    "fig, axl = plt.subplots()\n",
    "axl.set_title(\"Category Predictions\")\n",
    "axl.set_ylabel(\"Count\")\n",
    "axl.set_xlabel(\"Category\")\n",
    "test_df.Category.value_counts().sort_index().plot(ax = axl, marker = '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Submission File </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "submission_df = test_df.loc[:, ['itemid', 'Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission_df file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
